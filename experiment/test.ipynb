{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tuannm/sonhoang/Vietnamese-OCR-from-scratch-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import OrderedDict\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthDataset(Dataset):\n",
    "    def __init__(self, opt):\n",
    "        super(SynthDataset, self).__init__()\n",
    "        self.path = os.path.join(opt['path'], opt['imgdir'])\n",
    "        self.images = os.listdir(self.path)\n",
    "        self.nSamples = len(self.images)\n",
    "        f = lambda x: os.path.join(self.path, x)\n",
    "        self.imagepaths = list(map(f, self.images))\n",
    "        transform_list = [\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, ), (0.5, ))\n",
    "        ]\n",
    "        self.transform = transforms.Compose(transform_list)\n",
    "        self.collate_fn = SynthCollator()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.nSamples\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        assert index <= len(self), 'index range error'\n",
    "        imagepath = self.imagepaths[index]\n",
    "        imagefile = os.path.basename(imagepath)\n",
    "        img = Image.open(imagepath)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        item = {'img': img, 'idx': index}\n",
    "        item['label'] = imagefile.split('_')[0]\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthCollator(object):\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        width = [item['img'].shape[2] for item in batch]\n",
    "        indexes = [item['idx'] for item in batch]\n",
    "        imgs = torch.ones([len(batch), batch[0]['img'].shape[0], batch[0]['img'].shape[1],\n",
    "                           max(width)], dtype=torch.float32)\n",
    "        \n",
    "        for idx, item in enumerate(batch):\n",
    "            try:\n",
    "                imgs[idx, :, :, 0:item['img'].shape[2]] = item['img']\n",
    "            except:\n",
    "                print(imgs.shape)\n",
    "                \n",
    "        item = {'img': imgs, 'idx': indexes}\n",
    "        if 'label' in batch[0].keys():\n",
    "            labels = [item['label'] for item in batch]\n",
    "            item['label']  = labels\n",
    "        \n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = \"AÀÁẢÃẠaàáảãạĂẰẮẲẴẶăằắẳẵặÂẦẤẨẪẬâầấẩẫậBbCcDdĐđEÈÉẺẼẸeèéẻẽẹÊỀẾỂỄỆêềếểễệGgHhIÌÍỈĨỊiìíỉĩịKkLlMmNnOÒÓỎÕỌoòóỏõọÔỒỐỔỖỘôồốổỗộƠỜỚỞỠỢơờớởỡợPpQqRrSsTtUÙÚỦŨỤuùúủũụƯỪỨỬỮỰưừứửữựVvXxYỲÝỶỸỴyỳýỷỹỵZz0123456789jJwWfF\"\n",
    "\n",
    "args = {\n",
    "    'name':'exp1',\n",
    "    'path':'data',\n",
    "    'imgdir': 'train',\n",
    "    'imgH':32,\n",
    "    'nChannels':1,\n",
    "    'nHidden':256,\n",
    "    'nClasses':len(alphabet),\n",
    "    'lr':0.001,\n",
    "    'epochs':4,\n",
    "    'batch_size':32,\n",
    "    'save_dir':'checkpoints',\n",
    "    'log_dir':'logs',\n",
    "    'resume':False,\n",
    "    'cuda':False,\n",
    "    'schedule':False\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BidirectionalLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, nIn, nHidden, nOut):\n",
    "        super(BidirectionalLSTM, self).__init__()\n",
    "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
    "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
    "    def forward(self, input):\n",
    "        self.rnn.flatten_parameters()\n",
    "        recurrent, _ = self.rnn(input)\n",
    "        T, b, h = recurrent.size()\n",
    "        t_rec = recurrent.view(T * b, h)\n",
    "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, opt, leakyRelu=False):\n",
    "        super(CRNN, self).__init__()\n",
    "\n",
    "        assert opt['imgH'] % 16 == 0, 'imgH has to be a multiple of 16'\n",
    "\n",
    "        ks = [3, 3, 3, 3, 3, 3, 2]\n",
    "        ps = [1, 1, 1, 1, 1, 1, 0]\n",
    "        ss = [1, 1, 1, 1, 1, 1, 1]\n",
    "        nm = [64, 128, 256, 256, 512, 512, 512]\n",
    "\n",
    "        cnn = nn.Sequential()\n",
    "\n",
    "        def convRelu(i, batchNormalization=False):\n",
    "            nIn = opt['nChannels'] if i == 0 else nm[i - 1]\n",
    "            nOut = nm[i]\n",
    "            cnn.add_module('conv{0}'.format(i),\n",
    "                           nn.Conv2d(nIn, nOut, ks[i], ss[i], ps[i]))\n",
    "            if batchNormalization:\n",
    "                cnn.add_module('batchnorm{0}'.format(i), nn.BatchNorm2d(nOut))\n",
    "            if leakyRelu:\n",
    "                cnn.add_module('relu{0}'.format(i),\n",
    "                               nn.LeakyReLU(0.2, inplace=True))\n",
    "            else:\n",
    "                cnn.add_module('relu{0}'.format(i), nn.ReLU(True))\n",
    "\n",
    "        convRelu(0)\n",
    "        cnn.add_module('pooling{0}'.format(0), nn.MaxPool2d(2, 2))  # 64x16x64\n",
    "        convRelu(1)\n",
    "        cnn.add_module('pooling{0}'.format(1), nn.MaxPool2d(2, 2))  # 128x8x32\n",
    "        convRelu(2, True)\n",
    "        convRelu(3)\n",
    "        cnn.add_module('pooling{0}'.format(2),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 256x4x16\n",
    "        convRelu(4, True)\n",
    "        convRelu(5)\n",
    "        cnn.add_module('pooling{0}'.format(3),\n",
    "                       nn.MaxPool2d((2, 2), (2, 1), (0, 1)))  # 512x2x16\n",
    "        convRelu(6, True)  # 512x1x16\n",
    "        self.cnn = cnn\n",
    "        self.rnn = nn.Sequential()\n",
    "        self.rnn = nn.Sequential(\n",
    "            BidirectionalLSTM(opt['nHidden']*2, opt['nHidden'], opt['nHidden']),\n",
    "            BidirectionalLSTM(opt['nHidden'], opt['nHidden'], opt['nClasses']))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # conv features\n",
    "        conv = self.cnn(input)\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, \"the height of conv must be 1\"\n",
    "        conv = conv.squeeze(2)\n",
    "        conv = conv.permute(2, 0, 1)  # [w, b, c]\n",
    "        # rnn features\n",
    "        output = self.rnn(conv)\n",
    "        output = output.transpose(1,0) #Tbh to bth\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_out = model.cnn(data[10]['img'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 512, 1, 17]),\n",
       " tensor([[[[0.3934, 2.0215, 1.1573,  ..., 0.0000, 0.0000, 0.0351]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.4576,  ..., 0.0000, 0.0000, 1.5227]],\n",
       " \n",
       "          [[0.0000, 1.1980, 2.6751,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[1.5306, 0.2778, 0.0000,  ..., 2.1162, 1.7593, 0.0000]],\n",
       " \n",
       "          [[0.0000, 0.0000, 0.0000,  ..., 2.4536, 1.3614, 0.5925]],\n",
       " \n",
       "          [[0.0135, 0.0000, 0.0000,  ..., 0.2320, 0.8973, 0.0000]]]],\n",
       "        grad_fn=<ReluBackward0>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_out.size(), cnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3934, 2.0215, 1.1573, 0.0000, 0.0000, 0.0000, 0.1979, 0.0871, 0.2027,\n",
       "         0.4868, 0.7580, 0.4469, 0.0000, 0.0000, 0.0000, 0.0000, 0.0351]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_out[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_permute = cnn_out.squeeze(2).permute(2, 0 ,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([17, 1, 512]),\n",
       " tensor([[[0.3934, 0.0000, 0.0000,  ..., 1.5306, 0.0000, 0.0135]],\n",
       " \n",
       "         [[2.0215, 0.0000, 1.1980,  ..., 0.2778, 0.0000, 0.0000]],\n",
       " \n",
       "         [[1.1573, 0.4576, 2.6751,  ..., 0.0000, 0.0000, 0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 2.1162, 2.4536, 0.2320]],\n",
       " \n",
       "         [[0.0000, 0.0000, 0.0000,  ..., 1.7593, 1.3614, 0.8973]],\n",
       " \n",
       "         [[0.0351, 1.5227, 0.0000,  ..., 0.0000, 0.5925, 0.0000]]],\n",
       "        grad_fn=<PermuteBackward0>))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_permute.size(), cnn_permute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['alphabet'] = alphabet\n",
    "model = CRNN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SynthDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]['img'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRNN(\n",
       "  (cnn): Sequential(\n",
       "    (conv0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu0): ReLU(inplace=True)\n",
       "    (pooling0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu1): ReLU(inplace=True)\n",
       "    (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (conv2): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu2): ReLU(inplace=True)\n",
       "    (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu3): ReLU(inplace=True)\n",
       "    (pooling2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (batchnorm4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu4): ReLU(inplace=True)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (relu5): ReLU(inplace=True)\n",
       "    (pooling3): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False)\n",
       "    (conv6): Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (batchnorm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu6): ReLU(inplace=True)\n",
       "  )\n",
       "  (rnn): Sequential(\n",
       "    (0): BidirectionalLSTM(\n",
       "      (rnn): LSTM(512, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (1): BidirectionalLSTM(\n",
       "      (rnn): LSTM(256, 256, bidirectional=True)\n",
       "      (embedding): Linear(in_features=512, out_features=197, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(data[10]['img'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 64])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[10]['img'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 1, 196])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.transpose(1, 0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.transpose(1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.nn.functional.log_softmax(out, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-5.3379, -5.2193, -5.2672,  ..., -5.2889, -5.2489, -5.2710]],\n",
       " \n",
       "         [[-5.3359, -5.2211, -5.2689,  ..., -5.2991, -5.2500, -5.2812]],\n",
       " \n",
       "         [[-5.3356, -5.2243, -5.2651,  ..., -5.3010, -5.2502, -5.2842]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-5.3185, -5.2342, -5.2566,  ..., -5.3004, -5.2426, -5.2962]],\n",
       " \n",
       "         [[-5.3188, -5.2425, -5.2545,  ..., -5.3087, -5.2352, -5.2978]],\n",
       " \n",
       "         [[-5.3171, -5.2432, -5.2548,  ..., -5.3100, -5.2435, -5.2845]]],\n",
       "        grad_fn=<LogSoftmaxBackward0>),\n",
       " torch.Size([17, 1, 196]))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits, logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.1980],\n",
       "         [-5.1909],\n",
       "         [-5.1935],\n",
       "         [-5.1910],\n",
       "         [-5.1870],\n",
       "         [-5.1836],\n",
       "         [-5.1813],\n",
       "         [-5.1882],\n",
       "         [-5.1907],\n",
       "         [-5.1940],\n",
       "         [-5.1962],\n",
       "         [-5.1926],\n",
       "         [-5.1883],\n",
       "         [-5.1813],\n",
       "         [-5.1814],\n",
       "         [-5.1887],\n",
       "         [-5.2074]], grad_fn=<MaxBackward0>),\n",
       " tensor([[ 79],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [150],\n",
       "         [150],\n",
       "         [150],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [164],\n",
       "         [ 64]]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs, preds = logits.max(2)\n",
    "probs, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet[79-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_preds = converter.decode(preds.data, pred_sizes.data, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ivvvvụụụvvvvvvvvề'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "T, B, H = logits.size()\n",
    "pred_sizes = torch.LongTensor([T for i in range(B)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.utils import OCRLabelConverter\n",
    "\n",
    "converter = OCRLabelConverter(args[\"alphabet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets, lenghts = converter.encode([data[10]['label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 40,  72, 147,   7], dtype=torch.int32),\n",
       " tensor([4], dtype=torch.int32))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets, lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets.unsqueeze(0).view(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.criterions.ctc import CustomCTCLoss\n",
    "\n",
    "criterion = CustomCTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(logits, targets.unsqueeze(0).view(-1), pred_sizes, lenghts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
